{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKgX/nP1K9yK4vBcNo+060",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariamAghayan/M_Aghayan_MLE_EPAM8/blob/main/MAghayan_HW5_P2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tX6Qshh7fZv",
        "outputId": "d963739e-89de-4fa0-a944-89d5e2342982"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial point: [-1.2  1. ]\n",
            "Newton's Method: [0.9999957  0.99999139] Iterations: 5\n",
            "Steepest Descent: [0.32726277 0.1040128 ] Iterations: 1000\n",
            "Conjugate Gradient: [1.00000012 1.00000025] Iterations: 125\n",
            "\n",
            "Initial point: [0. 0.]\n",
            "Newton's Method: [1. 1.] Iterations: 2\n",
            "Steepest Descent: [0.67388605 0.45255952] Iterations: 1000\n",
            "Conjugate Gradient: [1.         1.00000001] Iterations: 39\n",
            "\n",
            "Initial point: [1. 1.]\n",
            "Newton's Method: [1. 1.] Iterations: 0\n",
            "Steepest Descent: [1. 1.] Iterations: 0\n",
            "Conjugate Gradient: [1. 1.] Iterations: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Rosenbrock function, derivatives\n",
        "def rosenbrock(x):\n",
        "    return 100*(x[1] - x[0]**2)**2 + (1 - x[0])**2\n",
        "\n",
        "def rosenbrock_grad(x):\n",
        "    grad = np.zeros_like(x)\n",
        "    grad[0] = -400 * x[0] * (x[1] - x[0]**2) - 2 * (1 - x[0])\n",
        "    grad[1] = 200 * (x[1] - x[0]**2)\n",
        "    return grad\n",
        "\n",
        "def rosenbrock_hessian(x):\n",
        "    hessian = np.zeros((2, 2))\n",
        "    hessian[0, 0] = 1200*x[0]**2 - 400*x[1] + 2\n",
        "    hessian[0, 1] = -400*x[0]\n",
        "    hessian[1, 0] = -400*x[0]\n",
        "    hessian[1, 1] = 200\n",
        "    return hessian\n",
        "\n",
        "# Newton's Method\n",
        "def newtons_method(x0, tol=1e-5, max_iter=1000):\n",
        "    x = x0\n",
        "    for i in range(max_iter):\n",
        "        grad = rosenbrock_grad(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            return x, i\n",
        "        hessian = rosenbrock_hessian(x)\n",
        "        step = np.linalg.solve(hessian, -grad)\n",
        "        x = x + step\n",
        "    return x, max_iter\n",
        "\n",
        "# Steepest Descent Method\n",
        "def steepest_descent(x0, tol=1e-5, max_iter=1000, alpha=0.001):\n",
        "    x = x0\n",
        "    for i in range(max_iter):\n",
        "        grad = rosenbrock_grad(x)\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            return x, i\n",
        "        x = x - alpha * grad\n",
        "    return x, max_iter\n",
        "\n",
        "# Conjugate Gradient Method\n",
        "def conjugate_gradient(x0, tol=1e-5, max_iter=1000):\n",
        "    x = x0\n",
        "    grad = rosenbrock_grad(x)\n",
        "    d = -grad\n",
        "    for i in range(max_iter):\n",
        "        if np.linalg.norm(grad) < tol:\n",
        "            return x, i\n",
        "        alpha = -np.dot(grad, d) / np.dot(d, np.dot(rosenbrock_hessian(x), d))\n",
        "        x = x + alpha * d\n",
        "        new_grad = rosenbrock_grad(x)\n",
        "        beta = np.dot(new_grad, new_grad - grad) / np.dot(grad, grad)\n",
        "        d = -new_grad + beta * d\n",
        "        grad = new_grad\n",
        "    return x, max_iter\n",
        "\n",
        "# Testing the functions\n",
        "initial_points = [np.array([-1.2, 1.0]), np.array([0.0, 0.0]), np.array([1.0, 1.0])]\n",
        "\n",
        "for x0 in initial_points:\n",
        "    print(\"Initial point:\", x0)\n",
        "    x_newton, iter_newton = newtons_method(x0)\n",
        "    print(\"Newton's Method:\", x_newton, \"Iterations:\", iter_newton)\n",
        "\n",
        "    x_steepest, iter_steepest = steepest_descent(x0)\n",
        "    print(\"Steepest Descent:\", x_steepest, \"Iterations:\", iter_steepest)\n",
        "\n",
        "    x_conjugate, iter_conjugate = conjugate_gradient(x0)\n",
        "    print(\"Conjugate Gradient:\", x_conjugate, \"Iterations:\", iter_conjugate)\n",
        "    print()"
      ]
    }
  ]
}